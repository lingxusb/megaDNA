# megaDNA: A long-context language model for the generation of bacteriophage genomes
![Online_figure](https://github.com/lingxusb/megaDNA/assets/12596418/ef85a641-0a79-4232-9d09-4abf498f04be)

Generative pre-trained transformers (GPTs) have revolutionized the field of natural language processing. Inspired by this success, we develop a long-context generative model for genomes. Our multiscale transformer model was pre-trained on unannotated bacteriophage genome with byte-level tokenization. It generates de novo sequences up to 96K with functional genomic structure, including regulatory elements and novel proteins with phage-related functions. 

### Install

`megaDNA` is inheritated from [`MEGABYTE`](https://github.com/lucidrains/MEGABYTE-pytorch), the user should install `MEGABYTE-pytorch`, in addition to `beartype`, `einops` and `pytorch`. See requirements.txt for more details.

To install `megaDNA`, run the following bash script:
```bash
git clone https://github.com/lingxusb/megaDNA.git
cd megaDNA
pip install .
```

### Usage
```python
import torch
from megaDNA import MEGADNA
device = 'cpu'

model = MEGADNA(
    num_tokens = 6,
    dim = (512, 256, 196),
    depth = (8, 8, 8),
    max_seq_len = (128, 64, 16)
).to(device)

# To train the model:
sequence = torch.tensor(np.random.choice(np.arange(1,5), 2048)).long().to(device)[None,]
model(sequence, return_value='loss')
loss.backward()

# After training, new sequences can be generated by a primer sequence:

nucleotides = ['**', 'A', 'T', 'C', 'G', '#'] # vocabulary

seq_tokenized = model.generate(primer_sequence,
                               seq_len=context_length,
                               temperature=0.95, 
                               filter_thres=0.0)

# To transform tokens back to DNA ucleotide sequence:
def token2nucleotide(s):
    return nucleotides[s]
generated_sequence = ''.join(map(token2nucleotide, seq_tokenized.squeeze().cpu().int()))
```

#### Trained model
The trained 145M model is availale at [huggingface](https://huggingface.co/lingxusb/megaDNA_145M/tree/main)

#### Model inference
jupyter notebook: [megaDNA_generate.ipynb](https://github.com/lingxusb/megaDNA/blob/main/megaDNA_generate.ipynb). GPU recommended.

Or you can easily run the [Colab Notebook](https://colab.research.google.com/drive/1T7pDY-pL2aJk8mogUKhDu5DpG9r7bjv4?usp=sharing) in the browser. Please make sure to connect to a GPU instance.

### Reference
- [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/2305.07185)
- [MEGABYTE-pytorch by Phil Wang](https://github.com/lucidrains/MEGABYTE-pytorch)
- Please contact shaobinlx@gmail.com or raise an issue in the github repo with any questions.
